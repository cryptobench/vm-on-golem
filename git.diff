diff --git a/provider-server/README.md b/provider-server/README.md
index 5517c63..3055371 100644
--- a/provider-server/README.md
+++ b/provider-server/README.md
@@ -235,7 +235,11 @@ Response:
 ### Starting the Provider
 
 ```bash
-poetry run python run.py
+# To run in production mode
+poetry run golem-provider start
+
+# To run in development mode
+poetry run golem-provider dev
 ```
 
 The provider will:
diff --git a/provider-server/poetry.lock b/provider-server/poetry.lock
index dbe17f1..38a5aa1 100644
--- a/provider-server/poetry.lock
+++ b/provider-server/poetry.lock
@@ -968,6 +968,38 @@ toolz = ">=0.8.0"
 [package.extras]
 cython = ["cython"]
 
+[[package]]
+name = "dependency-injector"
+version = "4.48.1"
+description = "Dependency injection framework for Python"
+optional = false
+python-versions = ">=3.8"
+files = [
+    {file = "dependency_injector-4.48.1-cp38-abi3-macosx_11_0_arm64.whl", hash = "sha256:a6f73011d532f3ea59689aad85c7999be6da3f30393041a745d5861cdcdc02e4"},
+    {file = "dependency_injector-4.48.1-cp38-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl", hash = "sha256:ac09f508fa9aee06a036ebf3e3d3b2a210276aba1993e9993cec7f1fdc5fd89e"},
+    {file = "dependency_injector-4.48.1-cp38-abi3-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:1b05a4a980096b53ad90a87965c5450183bfbb8bbe36615d7cea97537086d622"},
+    {file = "dependency_injector-4.48.1-cp38-abi3-musllinux_1_2_aarch64.whl", hash = "sha256:0506e98440ee6c48fe660016d602961b1b3ecc0a8227838a2221048ed11e2fca"},
+    {file = "dependency_injector-4.48.1-cp38-abi3-musllinux_1_2_x86_64.whl", hash = "sha256:1994622eae8917138626303b176cba4c74e625ba1e588cb09d673ca175d299a2"},
+    {file = "dependency_injector-4.48.1-cp38-abi3-win32.whl", hash = "sha256:58d4d81f92e3267c331f160cbbb517fd7644b95ee57a0d6e4b01f53a7e437a4a"},
+    {file = "dependency_injector-4.48.1-cp38-abi3-win_amd64.whl", hash = "sha256:572b22b7db9b103718ea52634b5ca1ef763278338310254334f4633a57c9f0e7"},
+    {file = "dependency_injector-4.48.1-pp310-pypy310_pp73-macosx_11_0_arm64.whl", hash = "sha256:9a7862987b3dcab5ac4fd82f6bbda55d3b15af1ca7492757c428deccc3720140"},
+    {file = "dependency_injector-4.48.1-pp310-pypy310_pp73-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl", hash = "sha256:bb33c6d6b6100564dfaee20f3f76a2756158cceff6499e9d0bca8290f8e5f124"},
+    {file = "dependency_injector-4.48.1-pp310-pypy310_pp73-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:a659380386bd236579b7f82f51e97f6074d0c878a10db5b50086000de6ce3c28"},
+    {file = "dependency_injector-4.48.1-pp310-pypy310_pp73-win_amd64.whl", hash = "sha256:76774369c7268d5dd211af75abfcb4433d972760be90db342c2de325ee4c24a0"},
+    {file = "dependency_injector-4.48.1-pp311-pypy311_pp73-macosx_11_0_arm64.whl", hash = "sha256:51f8d9d78a1a147908ed7929df628d859251a814e6a001973bd96ae2b5648760"},
+    {file = "dependency_injector-4.48.1-pp311-pypy311_pp73-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl", hash = "sha256:2c23ab17cd3e160de1fc5d78719bf86fbfc81c21c8ea02b43832a6a1e2c8a8d8"},
+    {file = "dependency_injector-4.48.1-pp311-pypy311_pp73-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:79be912a0dedb1341b1400018defca6a9966fcb8d4a84b325623fa57d3c08171"},
+    {file = "dependency_injector-4.48.1-pp311-pypy311_pp73-win_amd64.whl", hash = "sha256:65d9cf9f4eb31f837ed387210158e0003a4509478de1cbdc56c8439232f22ecd"},
+    {file = "dependency_injector-4.48.1.tar.gz", hash = "sha256:1805185e4522effad6d5e348c255d27e80d3f8adc89701daf13d743367392978"},
+]
+
+[package.extras]
+aiohttp = ["aiohttp"]
+flask = ["flask"]
+pydantic = ["pydantic"]
+pydantic2 = ["pydantic-settings"]
+yaml = ["pyyaml"]
+
 [[package]]
 name = "dill"
 version = "0.4.0"
@@ -3071,4 +3103,4 @@ propcache = ">=0.2.1"
 [metadata]
 lock-version = "2.0"
 python-versions = "^3.13"
-content-hash = "193c9c2fa87484ffdea9baea4c5fc2a276fa809ec91a5f10a0423bb1768cb9fe"
+content-hash = "aad5049c0b10f573199d4f615f6b1272ef11cea29cdb1d418139852c8e379fa3"
diff --git a/provider-server/provider/api/models.py b/provider-server/provider/api/models.py
index 976f4ee..5b33cb2 100644
--- a/provider-server/provider/api/models.py
+++ b/provider-server/provider/api/models.py
@@ -1,4 +1,4 @@
-from pydantic import BaseModel, Field, validator
+from pydantic import BaseModel, Field, field_validator
 from typing import Dict, Optional, List, Any
 from datetime import datetime
 
@@ -18,14 +18,14 @@ class CreateVMRequest(BaseModel):
     ssh_key: str = Field(..., pattern="^(ssh-rsa|ssh-ed25519) ",
                          description="SSH public key for VM access")
 
-    @validator("name")
+    @field_validator("name")
     def validate_name(cls, v: str) -> str:
         """Validate VM name."""
         if "--" in v:
             raise ValueError("VM name cannot contain consecutive hyphens")
         return v
 
-    @validator("resources", pre=True)
+    @field_validator("resources", mode='before')
     def validate_resources(cls, v: Optional[Dict[str, Any]], values: Dict[str, Any]) -> VMResources:
         """Validate and set resources."""
         logger.debug(f"Validating resources input: {v}")
@@ -43,10 +43,10 @@ class CreateVMRequest(BaseModel):
                 return v
 
             # If size provided, use that
-            if "size" in values and values["size"] is not None:
-                result = VMResources.from_size(values["size"])
+            if "size" in values.data and values.data["size"] is not None:
+                result = VMResources.from_size(values.data["size"])
                 logger.debug(
-                    f"Created resources from size {values['size']}: {result}")
+                    f"Created resources from size {values.data['size']}: {result}")
                 return result
 
             # Only use defaults if nothing provided
@@ -57,7 +57,7 @@ class CreateVMRequest(BaseModel):
         except Exception as e:
             logger.error(f"Error validating resources: {e}")
             logger.error(f"Input value: {v}")
-            logger.error(f"Values dict: {values}")
+            logger.error(f"Values dict: {values.data}")
             raise ValueError(f"Invalid resource configuration: {str(e)}")
 
 
diff --git a/provider-server/provider/api/routes.py b/provider-server/provider/api/routes.py
index 6b2fe5a..dd3fde2 100644
--- a/provider-server/provider/api/routes.py
+++ b/provider-server/provider/api/routes.py
@@ -3,150 +3,144 @@ from typing import List
 from pathlib import Path
 from fastapi import APIRouter, HTTPException, Request
 
-from ..config import settings
-from ..utils.logging import setup_logger, PROCESS, SUCCESS
+from dependency_injector.wiring import inject, Provide
+from fastapi import APIRouter, HTTPException, Depends
+
+from ..config import Settings
+from ..container import Container
+from ..utils.logging import setup_logger
 from ..utils.ascii_art import vm_creation_animation, vm_status_change
-from ..vm.models import VMInfo, VMStatus, VMAccessInfo, VMConfig, VMResources
+from ..vm.models import VMInfo, VMAccessInfo, VMConfig, VMResources, VMNotFoundError
 from .models import CreateVMRequest
-from ..vm.multipass import MultipassProvider, MultipassError
+from ..vm.service import VMService
+from ..vm.multipass_adapter import MultipassError
 
 logger = setup_logger(__name__)
 router = APIRouter()
 
+
 @router.post("/vms", response_model=VMInfo)
-async def create_vm(request: CreateVMRequest, req: Request) -> VMInfo:
+@inject
+async def create_vm(
+    request: CreateVMRequest,
+    vm_service: VMService = Depends(Provide[Container.vm_service]),
+    settings: Settings = Depends(Provide[Container.config]),
+) -> VMInfo:
     """Create a new VM."""
     try:
         logger.info(f"📥 Received VM creation request for '{request.name}'")
         
-        # Determine resources based on request
-        resources = request.resources
-        if resources is None:
-            # This shouldn't happen due to validator, but just in case
-            resources = VMResources(cpu=1, memory=1, storage=10)
-            
-        logger.info(f"📥 Using resources: {resources.cpu} CPU, {resources.memory}GB RAM, {resources.storage}GB storage")
+        resources = request.resources or VMResources()
         
-        # Validate against minimum requirements
-        if resources.cpu < settings.MIN_CPU_CORES:
-            logger.error(f"❌ CPU cores {resources.cpu} below minimum {settings.MIN_CPU_CORES}")
-            raise HTTPException(400, f"Minimum CPU cores required: {settings.MIN_CPU_CORES}")
-        if resources.memory < settings.MIN_MEMORY_GB:
-            logger.error(f"❌ Memory {resources.memory}GB below minimum {settings.MIN_MEMORY_GB}GB")
-            raise HTTPException(400, f"Minimum memory required: {settings.MIN_MEMORY_GB}GB")
-        if resources.storage < settings.MIN_STORAGE_GB:
-            logger.error(f"❌ Storage {resources.storage}GB below minimum {settings.MIN_STORAGE_GB}GB")
-            raise HTTPException(400, f"Minimum storage required: {settings.MIN_STORAGE_GB}GB")
-
-        # Check and allocate resources
-        logger.process("🔄 Allocating resources")
-        if not await req.app.state.resource_tracker.allocate(resources):
-            logger.error("❌ Insufficient resources available")
-            raise HTTPException(400, "Insufficient resources available on provider")
-        
-        try:
-            # Create VM config
-            config = VMConfig(
-                name=request.name,
-                image=request.image or settings.DEFAULT_VM_IMAGE,
-                resources=resources,
-                ssh_key=request.ssh_key
-            )
-            
-            # Create VM
-            logger.process(f"🔄 Creating VM with config: {config}")
-            vm_info = await req.app.state.provider.create_vm(config)
-
-            # Show success message
-            await vm_creation_animation(request.name)
-            return vm_info
-        except Exception as e:
-            # If VM creation fails, deallocate resources
-            logger.warning("⚠️ VM creation failed, deallocating resources")
-            await req.app.state.resource_tracker.deallocate(resources)
-            raise
+        # Create VM config
+        config = VMConfig(
+            name=request.name,
+            image=request.image or settings["DEFAULT_VM_IMAGE"],
+            resources=resources,
+            ssh_key=request.ssh_key
+        )
         
+        vm_info = await vm_service.create_vm(config)
+        await vm_creation_animation(request.name)
+        return vm_info
     except MultipassError as e:
         logger.error(f"Failed to create VM: {e}")
-        raise HTTPException(500, str(e))
+        raise HTTPException(status_code=500, detail=str(e))
+    except Exception as e:
+        logger.error(f"An unexpected error occurred: {e}")
+        raise HTTPException(status_code=500, detail="An unexpected error occurred")
+
 
 @router.get("/vms", response_model=List[VMInfo])
-async def list_vms(req: Request) -> List[VMInfo]:
+@inject
+async def list_vms(
+    vm_service: VMService = Depends(Provide[Container.vm_service]),
+) -> List[VMInfo]:
     """List all VMs."""
     try:
         logger.info("📋 Listing all VMs")
-        vms = []
-        for vm_id in req.app.state.resource_tracker.get_allocated_vms():
-            vm_info = await req.app.state.provider.get_vm_status(vm_id)
-            vms.append(vm_info)
-        return vms
+        return await vm_service.list_vms()
     except MultipassError as e:
         logger.error(f"Failed to list VMs: {e}")
-        raise HTTPException(500, str(e))
+        raise HTTPException(status_code=500, detail=str(e))
+    except Exception as e:
+        logger.error(f"An unexpected error occurred: {e}")
+        raise HTTPException(status_code=500, detail="An unexpected error occurred")
+
 
 @router.get("/vms/{requestor_name}", response_model=VMInfo)
-async def get_vm_status(requestor_name: str, req: Request) -> VMInfo:
+@inject
+async def get_vm_status(
+    requestor_name: str,
+    vm_service: VMService = Depends(Provide[Container.vm_service]),
+) -> VMInfo:
     """Get VM status."""
     try:
         logger.info(f"🔍 Getting status for VM '{requestor_name}'")
-        status = await req.app.state.provider.get_vm_status(requestor_name)
+        status = await vm_service.get_vm_status(requestor_name)
         vm_status_change(requestor_name, status.status.value)
         return status
+    except VMNotFoundError as e:
+        logger.error(f"VM not found: {e}")
+        raise HTTPException(status_code=404, detail=str(e))
     except MultipassError as e:
         logger.error(f"Failed to get VM status: {e}")
-        raise HTTPException(500, str(e))
+        raise HTTPException(status_code=500, detail=str(e))
+    except Exception as e:
+        logger.error(f"An unexpected error occurred: {e}")
+        raise HTTPException(status_code=500, detail="An unexpected error occurred")
+
 
 @router.get("/vms/{requestor_name}/access", response_model=VMAccessInfo)
-async def get_vm_access(requestor_name: str, req: Request) -> VMAccessInfo:
+@inject
+async def get_vm_access(
+    requestor_name: str,
+    vm_service: VMService = Depends(Provide[Container.vm_service]),
+    settings: Settings = Depends(Provide[Container.config]),
+) -> VMAccessInfo:
     """Get VM access information."""
     try:
-        # Get VM info
-        vm = await req.app.state.provider.get_vm_status(requestor_name)
+        vm = await vm_service.get_vm_status(requestor_name)
         if not vm:
             raise HTTPException(404, "VM not found")
         
-        # Get multipass name from mapper
-        multipass_name = await req.app.state.provider.name_mapper.get_multipass_name(requestor_name)
+        multipass_name = await vm_service.name_mapper.get_multipass_name(requestor_name)
         if not multipass_name:
             raise HTTPException(404, "VM mapping not found")
         
-        # Return access info with both names
         return VMAccessInfo(
-            ssh_host=settings.PUBLIC_IP or "localhost",
-            ssh_port=vm.ssh_port or 22,
+            ssh_host=settings["PUBLIC_IP"] or "localhost",
+            ssh_port=vm.ssh_port,
             vm_id=requestor_name,
             multipass_name=multipass_name
         )
-        
     except MultipassError as e:
         logger.error(f"Failed to get VM access info: {e}")
-        raise HTTPException(500, str(e))
+        raise HTTPException(status_code=500, detail=str(e))
+    except Exception as e:
+        logger.error(f"An unexpected error occurred: {e}")
+        raise HTTPException(status_code=500, detail="An unexpected error occurred")
+
 
 @router.delete("/vms/{requestor_name}")
-async def delete_vm(requestor_name: str, req: Request) -> None:
-    """Delete a VM.
-    
-    Args:
-        requestor_name: Name of the VM as provided by requestor
-    """
+@inject
+async def delete_vm(
+    requestor_name: str,
+    vm_service: VMService = Depends(Provide[Container.vm_service]),
+) -> None:
+    """Delete a VM."""
     try:
         logger.process(f"🗑️  Deleting VM '{requestor_name}'")
-        
-        # Get multipass name from mapper
-        multipass_name = await req.app.state.provider.name_mapper.get_multipass_name(requestor_name)
-        if not multipass_name:
-            logger.warning(f"No multipass name found for VM '{requestor_name}' (may have been already deleted)")
-            return
-            
-        try:
-            vm_status_change(requestor_name, "STOPPING", "Cleanup in progress")
-            await req.app.state.provider.delete_vm(requestor_name)
-            vm_status_change(requestor_name, "TERMINATED", "Cleanup complete")
-            logger.success(f"✨ Successfully deleted VM '{requestor_name}'")
-        except MultipassError as e:
-            logger.error(f"Failed to delete VM: {e}")
-            raise HTTPException(500, str(e))
-            
-    except Exception as e:
+        vm_status_change(requestor_name, "STOPPING", "Cleanup in progress")
+        await vm_service.delete_vm(requestor_name)
+        vm_status_change(requestor_name, "TERMINATED", "Cleanup complete")
+        logger.success(f"✨ Successfully deleted VM '{requestor_name}'")
+    except VMNotFoundError as e:
+        logger.error(f"VM not found: {e}")
+        raise HTTPException(status_code=404, detail=str(e))
+    except MultipassError as e:
         logger.error(f"Failed to delete VM: {e}")
-        raise HTTPException(500, str(e))
+        raise HTTPException(status_code=500, detail=str(e))
+    except Exception as e:
+        logger.error(f"An unexpected error occurred: {e}")
+        raise HTTPException(status_code=500, detail="An unexpected error occurred")
diff --git a/provider-server/provider/config.py b/provider-server/provider/config.py
index 0b29e17..2876661 100644
--- a/provider-server/provider/config.py
+++ b/provider-server/provider/config.py
@@ -4,7 +4,7 @@ from typing import Optional
 import uuid
 
 from pydantic_settings import BaseSettings
-from pydantic import validator, Field
+from pydantic import field_validator, Field
 from .utils.logging import setup_logger
 
 logger = setup_logger(__name__)
@@ -24,10 +24,10 @@ class Settings(BaseSettings):
     def DEV_MODE(self) -> bool:
         return self.ENVIRONMENT == "development"
 
-    @validator("SKIP_PORT_VERIFICATION", always=True)
+    @field_validator("SKIP_PORT_VERIFICATION", mode='before')
     def set_skip_verification(cls, v: bool, values: dict) -> bool:
         """Set skip verification based on debug mode."""
-        return v or values.get("DEBUG", False)
+        return v or values.data.get("DEBUG", False)
 
     # Provider Settings
     PROVIDER_NAME: str = "golem-provider"
@@ -39,7 +39,7 @@ class Settings(BaseSettings):
     CAPTCHA_URL: str = "https://cap.gobas.me"
     CAPTCHA_API_KEY: str = "05381a2cef5e"
  
-    @validator("ETHEREUM_KEY_DIR", pre=True)
+    @field_validator("ETHEREUM_KEY_DIR", mode='before')
     def resolve_key_dir(cls, v: str) -> str:
         """Resolve Ethereum key directory path."""
         if not v:
@@ -49,7 +49,7 @@ class Settings(BaseSettings):
             path = Path.home() / path
         return str(path)
 
-    @validator("ETHEREUM_PRIVATE_KEY", always=True)
+    @field_validator("ETHEREUM_PRIVATE_KEY", mode='before')
     def get_private_key(cls, v: Optional[str], values: dict) -> str:
         """Get private key from key file if not provided."""
         from provider.security.ethereum import EthereumIdentity
@@ -57,17 +57,17 @@ class Settings(BaseSettings):
         if v:
             return v
         
-        key_dir = values.get("ETHEREUM_KEY_DIR")
+        key_dir = values.data.get("ETHEREUM_KEY_DIR")
         identity = EthereumIdentity(key_dir)
         _, private_key = identity.get_or_create_identity()
         return private_key
 
-    @validator("PROVIDER_ID", always=True)
+    @field_validator("PROVIDER_ID", mode='before')
     def get_provider_id(cls, v: str, values: dict) -> str:
         """Get provider ID from private key."""
         from eth_account import Account
 
-        private_key = values.get("ETHEREUM_PRIVATE_KEY")
+        private_key = values.data.get("ETHEREUM_PRIVATE_KEY")
         if not private_key:
             raise ValueError("ETHEREUM_PRIVATE_KEY is not set")
 
@@ -83,16 +83,16 @@ class Settings(BaseSettings):
         
         return provider_id_from_key
  
-    @validator("PROVIDER_NAME", always=True)
+    @field_validator("PROVIDER_NAME", mode='before')
     def set_provider_name(cls, v: str, values: dict) -> str:
         """Prefix provider name with DEVMODE if in development."""
-        if values.get("ENVIRONMENT") == "development":
+        if values.data.get("ENVIRONMENT") == "development":
             return f"DEVMODE-{v}"
         return v
  
     # Discovery Service Settings
     DISCOVERY_URL: str = "http://195.201.39.101:9001"
-    DISCOVERY_DRIVER: str = "golem-base" # or "legacy"
+    ADVERTISER_TYPE: str = "golem_base"  # or "discovery_server"
     ADVERTISEMENT_INTERVAL: int = 240  # seconds
 
     # Golem Base Settings
@@ -107,127 +107,30 @@ class Settings(BaseSettings):
     CLOUD_INIT_DIR: str = ""
     CLOUD_INIT_FALLBACK_DIR: str = ""  # Will be set to a temp directory if needed
 
-    @validator("CLOUD_INIT_DIR", pre=True)
+    @field_validator("CLOUD_INIT_DIR", mode='before')
     def resolve_cloud_init_dir(cls, v: str) -> str:
         """Resolve and create cloud-init directory path."""
         import platform
         import tempfile
         from .utils.setup import setup_cloud_init_dir, check_setup_needed, mark_setup_complete
         
-        def verify_dir_permissions(path: Path) -> bool:
-            """Verify directory has correct permissions and is accessible."""
-            try:
-                # Create test file
-                test_file = path / "permission_test"
-                test_file.write_text("test")
-                test_file.unlink()
-                return True
-            except Exception:
-                return False
-
         if v:
-            path = Path(v)
-            if not path.is_absolute():
-                path = Path.home() / path
-        else:
-            system = platform.system().lower()
-            # Try OS-specific paths first
-            if system == "linux" and Path("/snap/bin/multipass").exists():
-                # Linux with snap
-                path = Path("/var/snap/multipass/common/cloud-init")
-                
-                # Check if we need to set up permissions
-                if check_setup_needed():
-                    logger.info("First run detected, setting up cloud-init directory...")
-                    success, error = setup_cloud_init_dir(path)
-                    if success:
-                        logger.info("✓ Cloud-init directory setup complete")
-                        mark_setup_complete()
-                    else:
-                        logger.error(f"Failed to set up cloud-init directory: {error}")
-                        logger.error("\nTo fix this manually, run these commands:")
-                        logger.error("  sudo mkdir -p /var/snap/multipass/common/cloud-init")
-                        logger.error("  sudo chown -R $USER:$USER /var/snap/multipass/common/cloud-init")
-                        logger.error("  sudo chmod -R 755 /var/snap/multipass/common/cloud-init\n")
-                        # Fall back to user's home directory
-                        path = Path.home() / ".local" / "share" / "golem" / "provider" / "cloud-init"
-                
-            elif system == "linux":
-                # Linux without snap
-                path = Path.home() / ".local" / "share" / "golem" / "provider" / "cloud-init"
-            elif system == "darwin":
-                # macOS
-                path = Path.home() / "Library" / "Application Support" / "golem" / "provider" / "cloud-init"
-            elif system == "windows":
-                # Windows
-                path = Path(os.path.expandvars("%LOCALAPPDATA%")) / "golem" / "provider" / "cloud-init"
-            else:
-                path = Path.home() / ".golem" / "provider" / "cloud-init"
-
-        try:
-            # Try to create and verify the directory
-            path.mkdir(parents=True, exist_ok=True)
-            if platform.system().lower() != "windows":
-                path.chmod(0o755)  # Readable and executable by owner and others, writable by owner
-
-            if verify_dir_permissions(path):
-                logger.debug(f"Created cloud-init directory at {path}")
-                return str(path)
-            
-            # If verification fails, fall back to temp directory
-            fallback_path = Path(tempfile.gettempdir()) / "golem" / "cloud-init"
-            fallback_path.mkdir(parents=True, exist_ok=True)
-            if platform.system().lower() != "windows":
-                fallback_path.chmod(0o755)
-            
-            if verify_dir_permissions(fallback_path):
-                logger.warning(f"Using fallback cloud-init directory at {fallback_path}")
-                return str(fallback_path)
-            
-            raise ValueError("Could not create a writable cloud-init directory")
-            
-        except Exception as e:
-            logger.error(f"Failed to create cloud-init directory at {path}: {e}")
-            raise ValueError(f"Failed to create cloud-init directory: {e}")
+            return v
+        return str(Path.home() / ".golem" / "provider" / "cloud-init")
 
-    @validator("VM_DATA_DIR", pre=True)
+    @field_validator("VM_DATA_DIR", mode='before')
     def resolve_vm_data_dir(cls, v: str) -> str:
-        """Resolve and create VM data directory path."""
-        if not v:
-            path = Path.home() / ".golem" / "provider" / "vms"
-        else:
-            path = Path(v)
-            if not path.is_absolute():
-                path = Path.home() / path
-        
-        try:
-            path.mkdir(parents=True, exist_ok=True)
-            logger.debug(f"Created VM data directory at {path}")
-        except Exception as e:
-            logger.error(f"Failed to create VM data directory at {path}: {e}")
-            raise ValueError(f"Failed to create VM data directory: {e}")
-            
-        return str(path)
+        """Resolve VM data directory path."""
+        if v:
+            return v
+        return str(Path.home() / ".golem" / "provider" / "vms")
 
-    @validator("SSH_KEY_DIR", pre=True)
+    @field_validator("SSH_KEY_DIR", mode='before')
     def resolve_ssh_key_dir(cls, v: str) -> str:
-        """Resolve and create SSH key directory path with secure permissions."""
-        if not v:
-            path = Path.home() / ".golem" / "provider" / "ssh"
-        else:
-            path = Path(v)
-            if not path.is_absolute():
-                path = Path.home() / path
-        
-        try:
-            path.mkdir(parents=True, exist_ok=True)
-            path.chmod(0o700)  # Secure permissions for SSH keys
-            logger.debug(f"Created SSH key directory at {path} with secure permissions")
-        except Exception as e:
-            logger.error(f"Failed to create SSH key directory at {path}: {e}")
-            raise ValueError(f"Failed to create SSH key directory: {e}")
-            
-        return str(path)
+        """Resolve SSH key directory path."""
+        if v:
+            return v
+        return str(Path.home() / ".golem" / "provider" / "ssh")
 
     # Resource Settings
     MIN_MEMORY_GB: int = 1
@@ -248,7 +151,7 @@ class Settings(BaseSettings):
         description="Path to multipass binary"
     )
 
-    @validator("MULTIPASS_BINARY_PATH")
+    @field_validator("MULTIPASS_BINARY_PATH")
     def detect_multipass_path(cls, v: str) -> str:
         """Detect and validate Multipass binary path."""
         import platform
@@ -373,7 +276,7 @@ class Settings(BaseSettings):
     PROXY_STATE_DIR: str = ""
     PUBLIC_IP: Optional[str] = None
 
-    @validator("PROXY_STATE_DIR", pre=True)
+    @field_validator("PROXY_STATE_DIR", mode='before')
     def resolve_proxy_state_dir(cls, v: str) -> str:
         """Resolve and create proxy state directory path."""
         if not v:
@@ -392,20 +295,12 @@ class Settings(BaseSettings):
             
         return str(path)
 
-    @validator("PUBLIC_IP", pre=True)
+    @field_validator("PUBLIC_IP", mode='before')
     def get_public_ip(cls, v: Optional[str], values: dict) -> Optional[str]:
         """Get public IP if set to 'auto'."""
+        if values.data.get("ENVIRONMENT") == "development":
+            return "127.0.0.1"
         if v == "auto":
-            if values.get("ENVIRONMENT") == "development":
-                import socket
-                try:
-                    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
-                    s.connect(("8.8.8.8", 80))
-                    local_ip = s.getsockname()[0]
-                    s.close()
-                    return local_ip
-                except Exception:
-                    return "127.0.0.1"
             try:
                 import requests
                 response = requests.get("https://api.ipify.org")
diff --git a/provider-server/provider/discovery/__init__.py b/provider-server/provider/discovery/__init__.py
index 0739b6f..bd0e5ac 100644
--- a/provider-server/provider/discovery/__init__.py
+++ b/provider-server/provider/discovery/__init__.py
@@ -1,6 +1,12 @@
-from .advertiser import ResourceMonitor, ResourceAdvertiser
+from .advertiser import Advertiser, DiscoveryServerAdvertiser
+from .golem_base_advertiser import GolemBaseAdvertiser
+from .resource_monitor import ResourceMonitor
+from .service import AdvertisementService
 
 __all__ = [
+    "Advertiser",
+    "DiscoveryServerAdvertiser",
+    "GolemBaseAdvertiser",
     "ResourceMonitor",
-    "ResourceAdvertiser"
+    "AdvertisementService",
 ]
diff --git a/provider-server/provider/discovery/advertiser.py b/provider-server/provider/discovery/advertiser.py
index e10b87a..45f2a29 100644
--- a/provider-server/provider/discovery/advertiser.py
+++ b/provider-server/provider/discovery/advertiser.py
@@ -1,90 +1,71 @@
 import aiohttp
 import asyncio
 import logging
-import psutil
-from datetime import datetime
-from typing import Dict, Optional
+from abc import ABC, abstractmethod
+from typing import Optional
 
 from ..config import settings
 from ..utils.retry import async_retry
+from .resource_tracker import ResourceTracker
 
 logger = logging.getLogger(__name__)
 
-class ResourceMonitor:
-    """Monitor system resources."""
-    
-    @staticmethod
-    def get_cpu_count() -> int:
-        """Get number of CPU cores."""
-        return psutil.cpu_count()
-
-    @staticmethod
-    def get_memory_gb() -> int:
-        """Get available memory in GB."""
-        return psutil.virtual_memory().available // (1024 ** 3)
-
-    @staticmethod
-    def get_storage_gb() -> int:
-        """Get available storage in GB."""
-        return psutil.disk_usage("/").free // (1024 ** 3)
-
-    @staticmethod
-    def get_cpu_percent() -> float:
-        """Get CPU usage percentage."""
-        return psutil.cpu_percent(interval=1)
-
-    @staticmethod
-    def get_memory_percent() -> float:
-        """Get memory usage percentage."""
-        return psutil.virtual_memory().percent
-
-    @staticmethod
-    def get_storage_percent() -> float:
-        """Get storage usage percentage."""
-        return psutil.disk_usage("/").percent
-
-class ResourceAdvertiser:
-    """Advertise available resources to discovery service."""
+class Advertiser(ABC):
+    """Abstract base class for advertisers."""
+
+    @abstractmethod
+    async def initialize(self):
+        """Initialize the advertiser."""
+        pass
+
+    @abstractmethod
+    async def start_loop(self):
+        """Start the advertising loop."""
+        pass
+
+    @abstractmethod
+    async def stop(self):
+        """Stop the advertising loop."""
+        pass
+
+    @abstractmethod
+    async def post_advertisement(self):
+        """Post a single advertisement."""
+        pass
+
+class DiscoveryServerAdvertiser(Advertiser):
+    """Advertise available resources to a discovery service."""
     
     def __init__(
         self,
         resource_tracker: 'ResourceTracker',
         discovery_url: Optional[str] = None,
         provider_id: Optional[str] = None,
-        update_interval: Optional[int] = None
     ):
         self.resource_tracker = resource_tracker
         self.discovery_url = discovery_url or settings.DISCOVERY_URL
         self.provider_id = provider_id or settings.PROVIDER_ID
-        self.update_interval = update_interval or settings.ADVERTISEMENT_INTERVAL
         self.session: Optional[aiohttp.ClientSession] = None
         self._stop_event = asyncio.Event()
 
-    async def start(self):
-        """Start advertising resources."""
+    async def initialize(self):
+        """Initialize the advertiser."""
         self.session = aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10))
-        # Register for resource updates
-        self.resource_tracker.on_update(self._post_advertisement)
-        
-        # Test discovery service connection with retries
+        self.resource_tracker.on_update(
+            lambda: asyncio.create_task(self.post_advertisement())
+        )
         try:
             await self._check_discovery_health()
         except Exception as e:
             logger.warning(f"Could not connect to discovery service after retries, continuing without advertising: {e}")
             return
-            
+
+    async def start_loop(self):
+        """Start advertising resources in a loop."""
         try:
             while not self._stop_event.is_set():
-                try:
-                    await self._post_advertisement()
-                except aiohttp.ClientError as e:
-                    logger.error(f"Network error posting advertisement: {e}")
-                    await asyncio.sleep(min(60, self.update_interval))
-                except Exception as e:
-                    logger.error(f"Failed to post advertisement: {e}")
-                    await asyncio.sleep(min(60, self.update_interval))
-                else:
-                    await asyncio.sleep(self.update_interval)
+                await self.post_advertisement()
+                await asyncio.sleep(settings.ADVERTISEMENT_INTERVAL)
         finally:
             await self.stop()
 
@@ -106,19 +87,17 @@ class ResourceAdvertiser:
                 raise Exception(f"Discovery service health check failed: {response.status}")
 
     @async_retry(retries=3, delay=1.0, backoff=2.0, exceptions=(aiohttp.ClientError, asyncio.TimeoutError))
-    async def _post_advertisement(self):
+    async def post_advertisement(self):
         """Post resource advertisement to discovery service."""
         if not self.session:
             raise RuntimeError("Session not initialized")
 
         resources = self.resource_tracker.get_available_resources()
         
-        # Don't advertise if resources are too low
         if not self.resource_tracker._meets_minimum_requirements(resources):
             logger.warning("Resources too low, skipping advertisement")
             return
 
-        # Get public IP with retries
         try:
             ip_address = await self._get_public_ip()
         except Exception as e:
@@ -130,7 +109,7 @@ class ResourceAdvertiser:
                 f"{self.discovery_url}/api/v1/advertisements",
                 headers={
                     "X-Provider-ID": self.provider_id,
-                    "X-Provider-Signature": "signature",  # TODO: Implement signing
+                    "X-Provider-Signature": "signature",
                     "Content-Type": "application/json"
                 },
                 json={
@@ -138,7 +117,7 @@ class ResourceAdvertiser:
                     "country": settings.PROVIDER_COUNTRY,
                     "resources": resources
                 },
-                timeout=aiohttp.ClientTimeout(total=5)  # 5 second timeout for advertisement
+                timeout=aiohttp.ClientTimeout(total=5)
             ) as response:
                 if not response.ok:
                     error_text = await response.text()
@@ -159,7 +138,6 @@ class ResourceAdvertiser:
         if not self.session:
             raise RuntimeError("Session not initialized")
 
-        # Try multiple IP services in case one fails
         services = [
             "https://api.ipify.org",
             "https://ifconfig.me/ip",
diff --git a/provider-server/provider/discovery/golem_base_advertiser.py b/provider-server/provider/discovery/golem_base_advertiser.py
index fd11849..bca5a92 100644
--- a/provider-server/provider/discovery/golem_base_advertiser.py
+++ b/provider-server/provider/discovery/golem_base_advertiser.py
@@ -2,6 +2,7 @@ import asyncio
 from typing import Optional
 
 from golem_base_sdk import GolemBaseClient, GolemBaseCreate, GolemBaseUpdate, GolemBaseDelete, Annotation
+from .advertiser import Advertiser
 from .golem_base_utils import get_provider_entity_keys
 from ..config import settings
 from ..utils.logging import setup_logger
@@ -9,7 +10,7 @@ from ..utils.logging import setup_logger
 logger = setup_logger(__name__)
 
 
-class GolemBaseAdvertiser:
+class GolemBaseAdvertiser(Advertiser):
     """Advertise available resources to the Golem Base network."""
 
     def __init__(self, resource_tracker: "ResourceTracker"):
diff --git a/provider-server/provider/discovery/resource_tracker.py b/provider-server/provider/discovery/resource_tracker.py
index 3f00c7c..6dce8c9 100644
--- a/provider-server/provider/discovery/resource_tracker.py
+++ b/provider-server/provider/discovery/resource_tracker.py
@@ -11,7 +11,7 @@ class ResourceTracker:
 
     def __init__(self):
         """Initialize resource tracker."""
-        from .advertiser import ResourceMonitor
+        from .resource_monitor import ResourceMonitor
         self.total_resources = {
             "cpu": ResourceMonitor.get_cpu_count(),
             "memory": ResourceMonitor.get_memory_gb(),
diff --git a/provider-server/provider/main.py b/provider-server/provider/main.py
index 9e44453..62aeb1d 100644
--- a/provider-server/provider/main.py
+++ b/provider-server/provider/main.py
@@ -9,15 +9,35 @@ from .config import settings
 from .utils.logging import setup_logger, PROCESS, SUCCESS
 from .utils.ascii_art import startup_animation
 from .discovery.resource_tracker import ResourceTracker
-from .discovery.advertiser import ResourceAdvertiser
-from .discovery.golem_base_advertiser import GolemBaseAdvertiser
-from .vm.multipass import MultipassProvider
-from .vm.port_manager import PortManager
-from .security.faucet import FaucetClient
-
-logger = setup_logger(__name__)
+from .discovery.advertiser import DiscoveryServerAdvertiser
+from .container import Container
+from .service import ProviderService
+from .utils.logging import setup_logger
 
 app = FastAPI(title="VM on Golem Provider")
+container = Container()
+container.config.from_pydantic(settings)
+app.container = container
+container.wire(modules=[".api.routes"])
+
+from .vm.models import VMNotFoundError
+from fastapi import Request
+from fastapi.responses import JSONResponse
+
+@app.exception_handler(VMNotFoundError)
+async def vm_not_found_exception_handler(request: Request, exc: VMNotFoundError):
+    return JSONResponse(
+        status_code=404,
+        content={"message": str(exc)},
+    )
+
+@app.exception_handler(Exception)
+async def generic_exception_handler(request: Request, exc: Exception):
+    return JSONResponse(
+        status_code=500,
+        content={"message": "An unexpected error occurred"},
+    )
+
 # Add CORS middleware
 app.add_middleware(
     CORSMiddleware,
@@ -27,156 +47,18 @@ app.add_middleware(
     allow_headers=["*"],  # Allows all headers
 )
 
-
-async def setup_provider() -> None:
-    """Setup and initialize the provider components."""
-    try:
-        # Create resource tracker first
-        logger.process("🔄 Initializing resource tracker...")
-        resource_tracker = ResourceTracker()
-        app.state.resource_tracker = resource_tracker
-
-        # Create provider with resource tracker and temporary port manager
-        logger.process("🔄 Initializing VM provider...")
-        provider = MultipassProvider(
-            resource_tracker, port_manager=None)  # Will be set later
-
-        try:
-            # Initialize provider (without port operations)
-            await asyncio.wait_for(provider.initialize(), timeout=30)
-
-            # Store provider reference
-            app.state.provider = provider
-            app.state.proxy_manager = provider.proxy_manager
-
-            # Initialize port manager first to verify all ports
-            logger.process("🔄 Initializing port manager...")
-            port_manager = PortManager(
-                start_port=settings.PORT_RANGE_START,
-                end_port=settings.PORT_RANGE_END,
-                discovery_port=settings.PORT,
-                skip_verification=settings.SKIP_PORT_VERIFICATION
-            )
-
-            if not await port_manager.initialize():
-                raise RuntimeError("Port verification failed")
-
-            # Store port manager references
-            app.state.port_manager = port_manager
-            provider.port_manager = port_manager
-            app.state.proxy_manager.port_manager = port_manager
-
-            # Now restore proxy configurations using only verified ports
-            logger.process("🔄 Restoring proxy configurations...")
-            await app.state.proxy_manager._load_state()
-
-        except asyncio.TimeoutError:
-            logger.error("Provider initialization timed out")
-            raise
-        except Exception as e:
-            logger.error(f"Failed to initialize provider: {e}")
-            raise
-
-        # Create advertiser
-        logger.process("🔄 Initializing resource advertiser...")
-        if settings.DISCOVERY_DRIVER == "golem-base":
-            advertiser = GolemBaseAdvertiser(
-                resource_tracker=resource_tracker
-            )
-            await advertiser.initialize()
-        else:
-            advertiser = ResourceAdvertiser(
-                resource_tracker=resource_tracker,
-                discovery_url=settings.DISCOVERY_URL,
-                provider_id=settings.PROVIDER_ID
-            )
-        app.state.advertiser = advertiser
-
-        logger.success(
-            "✨ Provider setup complete and ready to accept requests")
-    except Exception as e:
-        logger.error(f"Failed to setup provider: {e}")
-        # Attempt cleanup of any initialized components
-        await cleanup_provider()
-        raise
-
-
-async def cleanup_provider() -> None:
-    """Cleanup provider components."""
-    cleanup_errors = []
-
-    # Stop advertiser
-    if hasattr(app.state, "advertiser"):
-        try:
-            await app.state.advertiser.stop()
-            if hasattr(app.state, "advertiser_task"):
-                app.state.advertiser_task.cancel()
-                try:
-                    await app.state.advertiser_task
-                except asyncio.CancelledError:
-                    pass
-        except Exception as e:
-            cleanup_errors.append(f"Failed to stop advertiser: {e}")
-
-    # Cleanup proxy manager first to stop all proxy servers
-    if hasattr(app.state, "proxy_manager"):
-        try:
-            await asyncio.wait_for(app.state.proxy_manager.cleanup(), timeout=30)
-        except asyncio.TimeoutError:
-            cleanup_errors.append("Proxy manager cleanup timed out")
-        except Exception as e:
-            cleanup_errors.append(f"Failed to cleanup proxy manager: {e}")
-
-    # Cleanup provider
-    if hasattr(app.state, "provider"):
-        try:
-            await asyncio.wait_for(app.state.provider.cleanup(), timeout=30)
-        except asyncio.TimeoutError:
-            cleanup_errors.append("Provider cleanup timed out")
-        except Exception as e:
-            cleanup_errors.append(f"Failed to cleanup provider: {e}")
-
-    if cleanup_errors:
-        error_msg = "\n".join(cleanup_errors)
-        logger.error(f"Errors during cleanup:\n{error_msg}")
-    else:
-        logger.success("✨ Provider cleanup complete")
-
-
 @app.on_event("startup")
 async def startup_event():
     """Handle application startup."""
-    try:
-        # Display startup animation
-        await startup_animation()
- 
-        # Initialize provider
-        await setup_provider()
-
-        # Check wallet balance and request funds if needed
-        faucet_client = FaucetClient(
-            faucet_url=settings.FAUCET_URL,
-            captcha_url=settings.CAPTCHA_URL,
-            captcha_api_key=settings.CAPTCHA_API_KEY,
-        )
-        await faucet_client.get_funds(settings.PROVIDER_ID)
-
-        # Post initial advertisement and start advertising loop
-        if isinstance(app.state.advertiser, GolemBaseAdvertiser):
-            await app.state.advertiser.post_advertisement()
-            app.state.advertiser_task = asyncio.create_task(app.state.advertiser.start_loop())
-
-    except Exception as e:
-        logger.error(f"Startup failed: {e}")
-        # Ensure proper cleanup
-        await cleanup_provider()
-        raise
+    provider_service = container.provider_service()
+    await provider_service.setup(app)
 
 
 @app.on_event("shutdown")
 async def shutdown_event():
     """Handle application shutdown."""
-    await cleanup_provider()
+    provider_service = container.provider_service()
+    await provider_service.cleanup()
 
 # Import routes after app creation to avoid circular imports
 app.include_router(routes.router, prefix="/api/v1")
@@ -232,21 +114,37 @@ cli = typer.Typer()
 @cli.command()
 def start(no_verify_port: bool = typer.Option(False, "--no-verify-port", help="Skip provider port verification.")):
     """Start the provider server."""
+    run_server(dev_mode=False, no_verify_port=no_verify_port)
+
+@cli.command()
+def dev(no_verify_port: bool = typer.Option(True, "--no-verify-port", help="Skip provider port verification.")):
+    """Start the provider server in development mode."""
+    run_server(dev_mode=True, no_verify_port=no_verify_port)
+
+def run_server(dev_mode: bool, no_verify_port: bool):
+    """Helper to run the uvicorn server."""
     import sys
     from pathlib import Path
     from dotenv import load_dotenv
     import uvicorn
     from .utils.logging import setup_logger
+    
+    # Load appropriate .env file
+    env_file = ".env.dev" if dev_mode else ".env"
+    env_path = Path(__file__).parent.parent / env_file
+    load_dotenv(dotenv_path=env_path)
+    
+    # In dev mode, force advertised IP to 127.0.0.1
+    if dev_mode:
+        os.environ["GOLEM_PROVIDER_PUBLIC_IP"] = "127.0.0.1"
+
+    # Import settings after loading env
     from .config import settings
 
     # Configure logging with debug mode
-    logger = setup_logger(__name__, debug=True)
+    logger = setup_logger(__name__, debug=dev_mode)
 
     try:
-        # Load environment variables from .env file
-        env_path = Path(__file__).parent.parent / '.env'
-        load_dotenv(dotenv_path=env_path)
-
         # Log environment variables
         logger.info("Environment variables:")
         for key, value in os.environ.items():
@@ -275,7 +173,7 @@ def start(no_verify_port: bool = typer.Option(False, "--no-verify-port", help="S
             host=settings.HOST,
             port=settings.PORT,
             reload=settings.DEBUG,
-            log_level="info" if not settings.DEBUG else "debug",
+            log_level="debug" if dev_mode else "info",
             log_config=log_config,
             timeout_keep_alive=60,  # Increase keep-alive timeout
             limit_concurrency=100,  # Limit concurrent connections
diff --git a/provider-server/provider/utils/logging.py b/provider-server/provider/utils/logging.py
index b4ee29c..c69ee6f 100644
--- a/provider-server/provider/utils/logging.py
+++ b/provider-server/provider/utils/logging.py
@@ -39,42 +39,26 @@ def setup_logger(name: Optional[str] = None, debug: bool = False) -> logging.Log
         Configured logger instance
     """
     logger = logging.getLogger(name or __name__)
-    logger.handlers = []  # Clear existing handlers
-    
-    # Fancy handler for important logs
-    fancy_handler = colorlog.StreamHandler(sys.stdout)
-    fancy_formatter = colorlog.ColoredFormatter(
-        "%(log_color)s[%(asctime)s] %(message)s",
+    if logger.handlers:
+        return logger  # Already configured
+
+    handler = colorlog.StreamHandler(sys.stdout)
+    formatter = colorlog.ColoredFormatter(
+        "%(log_color)s[%(asctime)s] %(levelname)s: %(message)s",
         datefmt="%Y-%m-%d %H:%M:%S",
-        reset=True,
         log_colors={
+            'DEBUG': 'cyan',
             'INFO': 'green',
             'PROCESS': 'yellow',
             'WARNING': 'yellow',
             'SUCCESS': 'green,bold',
             'ERROR': 'red',
             'CRITICAL': 'red,bold',
-        },
-        secondary_log_colors={},
-        style='%'
+        }
     )
-    fancy_handler.setFormatter(fancy_formatter)
-    fancy_handler.addFilter(lambda record: record.levelno in [INFO, PROCESS, SUCCESS, WARNING, ERROR, CRITICAL])
-    logger.addHandler(fancy_handler)
-    
-    if debug:
-        # Debug handler for detailed logs
-        debug_handler = logging.StreamHandler(sys.stdout)
-        debug_formatter = logging.Formatter(
-            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
-            datefmt="%Y-%m-%d %H:%M:%S"
-        )
-        debug_handler.setFormatter(debug_formatter)
-        debug_handler.addFilter(lambda record: record.levelno == DEBUG)
-        logger.addHandler(debug_handler)
-        logger.setLevel(logging.DEBUG)
-    else:
-        logger.setLevel(logging.INFO)
+    handler.setFormatter(formatter)
+    logger.addHandler(handler)
+    logger.setLevel(logging.DEBUG if debug else logging.INFO)
     
     return logger
 
diff --git a/provider-server/provider/utils/port_display.py b/provider-server/provider/utils/port_display.py
index 8424672..e98f160 100644
--- a/provider-server/provider/utils/port_display.py
+++ b/provider-server/provider/utils/port_display.py
@@ -100,10 +100,8 @@ class PortVerificationDisplay:
         print("-------------------------")
         
         if self.skip_verification:
-            print("\n✅ All ports available in development mode")
-            print(f"└─ Port Range: {self.port_range_start}-{self.port_range_end}")
-            print("└─ Status: Port verification skipped")
-            print("└─ Note: Configure ports before deploying to production")
+            print("✅ Development Mode: Port verification skipped")
+            print(f"└─ Port Range: {self.port_range_start}-{self.port_range_end} assumed available")
             return
 
         await self.animate_verification("Scanning VM access ports...")
@@ -209,10 +207,10 @@ class PortVerificationDisplay:
         print("\n🎯 Current Status:", end=" ")
 
         if self.skip_verification:
-            print("Development Mode")
-            print("└─ Status: Port verification skipped")
-            print(f"└─ Available: All ports in range {self.port_range_start}-{self.port_range_end}")
-            print("└─ Note: This is for development only, configure ports in production")
+            print("✅ Development Mode")
+            print("└─ Status: Local port verification complete")
+            print(f"└─ Available: All ports in range {self.port_range_start}-{self.port_range_end} are assumed to be available locally")
+            print("└─ Note: External accessibility is not checked in dev mode")
             return
         
         if discovery_result is None:
diff --git a/provider-server/provider/vm/__init__.py b/provider-server/provider/vm/__init__.py
index 92a38c7..a1fac86 100644
--- a/provider-server/provider/vm/__init__.py
+++ b/provider-server/provider/vm/__init__.py
@@ -12,7 +12,7 @@ from .models import (
     VMStateError,
     ResourceError
 )
-from .multipass import MultipassProvider
+from .multipass_adapter import MultipassAdapter
 
 __all__ = [
     "VMConfig",
diff --git a/provider-server/provider/vm/models.py b/provider-server/provider/vm/models.py
index f1b4486..2dd6747 100644
--- a/provider-server/provider/vm/models.py
+++ b/provider-server/provider/vm/models.py
@@ -1,5 +1,5 @@
 from enum import Enum
-from pydantic import BaseModel, Field, validator
+from pydantic import BaseModel, Field, field_validator
 from typing import Dict, Optional
 from datetime import datetime
 
@@ -28,14 +28,14 @@ class VMResources(BaseModel):
     memory: int = Field(..., ge=1, description="Memory in GB")
     storage: int = Field(..., ge=10, description="Storage in GB")
 
-    @validator("cpu")
+    @field_validator("cpu")
     def validate_cpu(cls, v: int) -> int:
         """Validate CPU cores."""
         if v not in [1, 2, 4, 8, 16]:
             raise ValueError("CPU cores must be 1, 2, 4, 8, or 16")
         return v
 
-    @validator("memory")
+    @field_validator("memory")
     def validate_memory(cls, v: int) -> int:
         """Validate memory."""
         if v not in [1, 2, 4, 8, 16, 32, 64]:
@@ -66,21 +66,21 @@ class VMCreateRequest(BaseModel):
     ssh_key: str = Field(..., pattern="^(ssh-rsa|ssh-ed25519) ",
                          description="SSH public key for VM access")
 
-    @validator("name")
+    @field_validator("name")
     def validate_name(cls, v: str) -> str:
         """Validate VM name."""
         if "--" in v:
             raise ValueError("VM name cannot contain consecutive hyphens")
         return v
 
-    @validator("cpu_cores")
+    @field_validator("cpu_cores")
     def validate_cpu(cls, v: Optional[int]) -> Optional[int]:
         """Validate CPU cores."""
         if v is not None and v not in [1, 2, 4, 8, 16]:
             raise ValueError("CPU cores must be 1, 2, 4, 8, or 16")
         return v
 
-    @validator("memory_gb")
+    @field_validator("memory_gb")
     def validate_memory(cls, v: Optional[int]) -> Optional[int]:
         """Validate memory."""
         if v is not None and v not in [1, 2, 4, 8, 16, 32, 64]:
@@ -97,8 +97,9 @@ class VMConfig(BaseModel):
     size: Optional[VMSize] = None
     ssh_key: str = Field(..., pattern="^(ssh-rsa|ssh-ed25519) ",
                          description="SSH public key for VM access")
+    cloud_init_path: Optional[str] = None
 
-    @validator("name")
+    @field_validator("name")
     def validate_name(cls, v: str) -> str:
         """Validate VM name."""
         if "--" in v:
diff --git a/provider-server/provider/vm/multipass.py b/provider-server/provider/vm/multipass.py
index 4bf8270..979745a 100644
--- a/provider-server/provider/vm/multipass.py
+++ b/provider-server/provider/vm/multipass.py
@@ -15,423 +15,5 @@ from .name_mapper import VMNameMapper
 logger = setup_logger(__name__)
 
 
-class MultipassError(VMError):
-    """Raised when multipass operations fail."""
-    pass
-
-
-class MultipassProvider(VMProvider):
-    """Manages VMs using Multipass."""
-
-    def __init__(self, resource_tracker: "ResourceTracker", port_manager: "PortManager"):
-        """Initialize the multipass provider.
-
-        Args:
-            resource_tracker: Resource tracker instance
-            port_manager: Port manager instance for SSH port allocation
-        """
-        self.resource_tracker = resource_tracker
-        self.port_manager = port_manager
-        self.multipass_path = settings.MULTIPASS_BINARY_PATH
-        self.vm_data_dir = Path(settings.VM_DATA_DIR)
-        self.vm_data_dir.mkdir(parents=True, exist_ok=True)
-
-        # Initialize managers
-        self.name_mapper = VMNameMapper(self.vm_data_dir / "vm_names.json")
-        self.proxy_manager = PythonProxyManager(
-            port_manager=port_manager,
-            name_mapper=self.name_mapper
-        )
-
-    def _verify_installation(self) -> None:
-        """Verify multipass is installed and get version."""
-        try:
-            result = subprocess.run(
-                [self.multipass_path, "version"],
-                capture_output=True,
-                text=True,
-                check=True
-            )
-            logger.info(f"🔧 Using Multipass version: {result.stdout.strip()}")
-        except subprocess.CalledProcessError as e:
-            raise MultipassError(
-                f"Failed to verify multipass installation: {e.stderr}")
-        except FileNotFoundError:
-            raise MultipassError(
-                f"Multipass not found at {self.multipass_path}")
-
-    def _get_all_vms_resources(self) -> Dict[str, VMResources]:
-        """Get resources for all running VMs from multipass.
-        
-        Returns:
-            Dictionary mapping VM names to their resources
-        """
-        result = self._run_multipass(["list", "--format", "json"])
-        data = json.loads(result.stdout)
-        vm_resources = {}
-        
-        for vm in data.get("list", []):
-            if vm.get("name", "").startswith("golem-"):
-                try:
-                    info = self._get_vm_info(vm["name"])
-                    vm_resources[vm["name"]] = VMResources(
-                        cpu=int(info.get("cpu_count", 1)),
-                        memory=int(info.get("memory_total", 1024) / 1024),
-                        storage=int(info.get("disk_total", 10 * 1024) / 1024)
-                    )
-                except Exception as e:
-                    logger.error(f"Failed to get info for VM {vm['name']}: {e}")
-                    continue
-        
-        return vm_resources
-
-    async def initialize(self) -> None:
-        """Initialize the provider."""
-        self._verify_installation()
-
-        # Create SSH key directory
-        ssh_key_dir = Path(settings.SSH_KEY_DIR)
-        ssh_key_dir.mkdir(parents=True, exist_ok=True)
-
-        # Sync resource tracker with actual multipass state
-        logger.info("🔄 Syncing resource tracker with multipass state...")
-        vm_resources = self._get_all_vms_resources()
-        await self.resource_tracker.sync_with_multipass(vm_resources)
-        logger.info("✨ Resource tracker synced with multipass state")
-
-    def _run_multipass(self, args: List[str], check: bool = True) -> subprocess.CompletedProcess:
-        """Run a multipass command.
-
-        Args:
-            args: Command arguments
-            check: Whether to check return code
-
-        Returns:
-            CompletedProcess instance
-        """
-        try:
-            return subprocess.run(
-                [self.multipass_path, *args],
-                capture_output=True,
-                text=True,
-                check=check
-            )
-        except subprocess.CalledProcessError as e:
-            raise MultipassError(f"Multipass command failed: {e.stderr}")
-
-    def _get_vm_info(self, vm_id: str) -> Dict:
-        """Get detailed information about a VM.
-
-        Args:
-            vm_id: VM identifier
-
-        Returns:
-            Dictionary with VM information
-        """
-        result = self._run_multipass(["info", vm_id, "--format", "json"])
-        try:
-            info = json.loads(result.stdout)
-            return info["info"][vm_id]
-        except (json.JSONDecodeError, KeyError) as e:
-            raise MultipassError(f"Failed to parse VM info: {e}")
-
-    def _get_vm_ip(self, vm_id: str) -> Optional[str]:
-        """Get IP address of a VM.
-
-        Args:
-            vm_id: VM identifier
-
-        Returns:
-            IP address or None if not found
-        """
-        try:
-            info = self._get_vm_info(vm_id)
-            return info.get("ipv4", [None])[0]
-        except Exception:
-            return None
-
-    async def create_vm(self, config: VMConfig) -> VMInfo:
-        """Create a new VM.
-
-        Args:
-            config: VM configuration
-
-        Returns:
-            Information about the created VM
-        """
-        multipass_name = f"golem-{config.name}-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
-        await self.name_mapper.add_mapping(config.name, multipass_name)
-        cloud_init_path = None
-        config_id = None
-
-        # Verify resources are properly allocated
-        if not self.resource_tracker.can_accept_resources(config.resources):
-            raise VMCreateError("Resources not properly allocated or insufficient")
-
-        try:
-            # Generate cloud-init config with requestor's public key
-            cloud_init_path, config_id = generate_cloud_init(
-                hostname=config.name,
-                ssh_key=config.ssh_key
-            )
-
-            # Launch VM
-            logger.process(f"🚀 Launching VM {multipass_name} with config {config_id}")
-            launch_cmd = [
-                "launch",
-                config.image,
-                "--name", multipass_name,
-                "--cloud-init", cloud_init_path,
-                "--cpus", str(config.resources.cpu),
-                "--memory", f"{config.resources.memory}G",
-                "--disk", f"{config.resources.storage}G"
-            ]
-            self._run_multipass(launch_cmd)
-
-            # Get VM IP
-            ip_address = self._get_vm_ip(multipass_name)
-            if not ip_address:
-                raise MultipassError("Failed to get VM IP address")
-
-            # Allocate port and configure proxy
-            try:
-                # First allocate a verified port
-                ssh_port = self.port_manager.allocate_port(multipass_name)
-                if not ssh_port:
-                    if settings.DEV_MODE:
-                        logger.warning("Failed to allocate verified SSH port in dev mode, falling back to random port")
-                        ssh_port = 0  # Let the proxy manager pick a random port
-                    else:
-                        raise MultipassError("Failed to allocate verified SSH port")
-
-                # Then configure proxy with allocated port
-                success = await self.proxy_manager.add_vm(multipass_name, ip_address, port=ssh_port)
-                if not success:
-                    # Clean up allocated port if proxy fails
-                    self.port_manager.deallocate_port(multipass_name)
-                    raise MultipassError("Failed to configure proxy")
-
-                # Create VM info and register with resource tracker
-                vm_info = VMInfo(
-                    id=config.name,  # Use requestor name as VM ID
-                    name=config.name,
-                    status=VMStatus.RUNNING,
-                    resources=config.resources,
-                    ip_address=ip_address,
-                    ssh_port=ssh_port
-                )
-
-                # Update resource tracker with VM ID
-                await self.resource_tracker.allocate(config.resources, config.name)
-
-                return vm_info
-
-            except Exception as e:
-                # If proxy configuration fails, ensure we cleanup the VM and resources
-                self._run_multipass(["delete", multipass_name, "--purge"], check=False)
-                await self.resource_tracker.deallocate(config.resources, config.name)
-                await self.name_mapper.remove_mapping(config.name)
-                raise VMCreateError(
-                    f"Failed to configure VM networking: {str(e)}", vm_id=config.name)
-
-        except Exception as e:
-            # Cleanup on failure (this catches VM creation errors)
-            try:
-                await self.delete_vm(config.name)
-            except Exception as cleanup_error:
-                logger.error(f"Error during VM cleanup: {cleanup_error}")
-            # Ensure resources are deallocated even if delete_vm fails
-            await self.resource_tracker.deallocate(config.resources, config.name)
-            raise VMCreateError(f"Failed to create VM: {str(e)}", vm_id=config.name)
-
-        finally:
-            # Cleanup cloud-init file
-            if cloud_init_path and config_id:
-                cleanup_cloud_init(cloud_init_path, config_id)
-
-    def _verify_vm_exists(self, vm_id: str) -> bool:
-        """Check if VM exists in multipass.
-        
-        Args:
-            vm_id: VM identifier
-            
-        Returns:
-            True if VM exists, False otherwise
-        """
-        try:
-            result = self._run_multipass(["list", "--format", "json"])
-            data = json.loads(result.stdout)
-            vms = data.get("list", [])
-            return any(vm.get("name") == vm_id for vm in vms)
-        except Exception:
-            return False
-
-    async def delete_vm(self, requestor_name: str) -> None:
-        """Delete a VM.
-
-        Args:
-            requestor_name: Requestor's VM name
-        """
-        # Get multipass name from mapper
-        multipass_name = await self.name_mapper.get_multipass_name(requestor_name)
-        if not multipass_name:
-            logger.warning(f"No multipass name found for VM {requestor_name}")
-            return
-
-        logger.process(f"🗑️  Initiating deletion of VM {multipass_name}")
-        
-        # Get VM info for resource deallocation
-        try:
-            vm_info = await self.get_vm_status(requestor_name)
-        except Exception as e:
-            logger.error(f"Failed to get VM info for cleanup: {e}")
-            vm_info = None
-
-        # Check if VM exists
-        if not self._verify_vm_exists(multipass_name):
-            logger.warning(f"VM {multipass_name} not found in multipass")
-        else:
-            try:
-                # First mark for deletion
-                logger.info("🔄 Marking VM for deletion...")
-                self._run_multipass(["delete", multipass_name], check=False)
-                
-                # Then purge
-                logger.info("🔄 Purging deleted VM...")
-                self._run_multipass(["purge"], check=False)
-                
-                # Verify deletion
-                if self._verify_vm_exists(multipass_name):
-                    logger.error(f"VM {multipass_name} still exists after deletion attempt")
-                    # Try one more time with force
-                    logger.info("🔄 Attempting forced deletion...")
-                    self._run_multipass(["stop", "--all", multipass_name], check=False)
-                    self._run_multipass(["delete", "--purge", multipass_name], check=False)
-                    if self._verify_vm_exists(multipass_name):
-                        raise MultipassError(f"Failed to delete VM {multipass_name}")
-                
-                logger.success("✨ VM instance removed")
-            except Exception as e:
-                logger.error(f"Error deleting VM {multipass_name} from multipass: {e}")
-                raise
-
-        # Clean up proxy config and port allocation
-        try:
-            logger.info("🔄 Cleaning up network configuration...")
-            await self.proxy_manager.remove_vm(multipass_name)
-            self.port_manager.deallocate_port(multipass_name)
-            logger.success("✨ Network configuration cleaned up")
-        except Exception as e:
-            logger.error(f"Error cleaning up network configuration for VM {multipass_name}: {e}")
-            
-        # Deallocate resources
-        if vm_info and vm_info.resources:
-            try:
-                logger.info("🔄 Deallocating resources...")
-                await self.resource_tracker.deallocate(vm_info.resources, requestor_name)
-                logger.success("✨ Resources deallocated")
-            except Exception as e:
-                logger.error(f"Error deallocating resources: {e}")
-
-        # Remove name mapping
-        try:
-            await self.name_mapper.remove_mapping(requestor_name)
-            logger.success("✨ Name mapping removed")
-        except Exception as e:
-            logger.error(f"Error removing name mapping: {e}")
-
-        # Sync resource tracker with actual state
-        logger.info("🔄 Syncing resource tracker with multipass state...")
-        vm_resources = self._get_all_vms_resources()
-        await self.resource_tracker.sync_with_multipass(vm_resources)
-        logger.info("✨ Resource tracker synced with multipass state")
-
-    async def start_vm(self, requestor_name: str) -> VMInfo:
-        """Start a VM.
-
-        Args:
-            requestor_name: Requestor's VM name
-
-        Returns:
-            Updated VM information
-        """
-        # Get multipass name from mapper
-        multipass_name = await self.name_mapper.get_multipass_name(requestor_name)
-        if not multipass_name:
-            raise VMNotFoundError(f"VM {requestor_name} not found")
-
-        logger.process(f"🔄 Starting VM '{requestor_name}'")
-        self._run_multipass(["start", multipass_name])
-        status = await self.get_vm_status(requestor_name)
-        logger.success(f"✨ VM '{requestor_name}' started successfully")
-        return status
-
-    async def stop_vm(self, requestor_name: str) -> VMInfo:
-        """Stop a VM.
-
-        Args:
-            requestor_name: Requestor's VM name
-
-        Returns:
-            Updated VM information
-        """
-        # Get multipass name from mapper
-        multipass_name = await self.name_mapper.get_multipass_name(requestor_name)
-        if not multipass_name:
-            raise VMNotFoundError(f"VM {requestor_name} not found")
-
-        logger.process(f"🔄 Stopping VM '{requestor_name}'")
-        self._run_multipass(["stop", multipass_name])
-        status = await self.get_vm_status(requestor_name)
-        logger.success(f"✨ VM '{requestor_name}' stopped successfully")
-        return status
-
-    async def get_vm_status(self, requestor_name: str) -> VMInfo:
-        """Get current status of a VM.
-
-        Args:
-            requestor_name: Requestor's VM name
-
-        Returns:
-            VM status information
-        """
-        try:
-            # Get multipass name from mapper
-            multipass_name = await self.name_mapper.get_multipass_name(requestor_name)
-            if not multipass_name:
-                raise VMNotFoundError(f"VM {requestor_name} not found")
-
-            # Get VM info from multipass
-            info = self._get_vm_info(multipass_name)
-
-            return VMInfo(
-                id=requestor_name,  # Use requestor name as ID
-                name=requestor_name,
-                status=VMStatus(info.get("state", "unknown").lower()),
-                resources=VMResources(
-                    cpu=int(info.get("cpu_count", 1)),
-                    memory=int(info.get("memory_total", 1024) / 1024),
-                    storage=int(info.get("disk_total", 10 * 1024) / 1024)
-                ),
-                ip_address=info.get("ipv4", [None])[0],
-                ssh_port=self.proxy_manager.get_port(multipass_name)
-            )
-        except Exception as e:
-            logger.error(f"Error getting VM status: {e}")
-            return VMInfo(
-                id=requestor_name,
-                name=requestor_name,
-                status=VMStatus.ERROR,
-                resources=VMResources(cpu=1, memory=1, storage=10),
-                error_message=str(e)
-            )
-
-    async def add_ssh_key(self, vm_id: str, key: str) -> None:
-        """Add SSH key to VM.
-
-        Args:
-            vm_id: VM identifier
-            key: SSH key to add
-        """
-        # Not implemented - we use cloud-init for SSH key setup
-        pass
+from .service import VMService
+from .multipass_adapter import MultipassAdapter
diff --git a/provider-server/provider/vm/name_mapper.py b/provider-server/provider/vm/name_mapper.py
index 102b264..c3ae7da 100644
--- a/provider-server/provider/vm/name_mapper.py
+++ b/provider-server/provider/vm/name_mapper.py
@@ -9,21 +9,21 @@ logger = logging.getLogger(__name__)
 class VMNameMapper:
     """Maps between requestor VM names and multipass VM names."""
 
-    def __init__(self, storage_path: Optional[Path] = None):
+    def __init__(self, db_path: Optional[Path] = None):
         """Initialize name mapper.
         
         Args:
-            storage_path: Optional path to persist mappings
+            db_path: Optional path to persist mappings
         """
         self._name_map: Dict[str, str] = {}  # requestor_name -> multipass_name
         self._reverse_map: Dict[str, str] = {}  # multipass_name -> requestor_name
         self._lock = asyncio.Lock()
-        self._storage_path = storage_path
+        self._storage_path = db_path
         
         # Load existing mappings if storage path provided
-        if storage_path and storage_path.exists():
+        if db_path and db_path.exists():
             try:
-                with open(storage_path) as f:
+                with open(db_path) as f:
                     data = json.load(f)
                     self._name_map = data.get('name_map', {})
                     self._reverse_map = data.get('reverse_map', {})
diff --git a/provider-server/provider/vm/port_manager.py b/provider-server/provider/vm/port_manager.py
index a36587c..37aaccc 100644
--- a/provider-server/provider/vm/port_manager.py
+++ b/provider-server/provider/vm/port_manager.py
@@ -47,10 +47,13 @@ class PortManager:
         self._existing_ports = existing_ports or set()
 
         # Initialize port verifier with default servers
-        self.port_check_servers = port_check_servers or [
-            "http://localhost:9000",  # Local development server
-            "http://195.201.39.101:9000",  # Production servers
-        ]
+        if settings.DEV_MODE:
+            self.port_check_servers = ["http://localhost:9000"]
+        else:
+            self.port_check_servers = port_check_servers or [
+                "http://localhost:9000",  # Local development server
+                "http://195.201.39.101:9000",  # Production servers
+            ]
         self.discovery_port = discovery_port or settings.PORT
         self.skip_verification = skip_verification
         self.port_verifier = PortVerifier(
@@ -105,7 +108,21 @@ class PortManager:
 
             # Clear existing verified ports before verification
             self.verified_ports.clear()
-            results = await self.port_verifier.verify_ports(ssh_ports)
+            results = {}
+            if not self.skip_verification:
+                try:
+                    results = await self.port_verifier.verify_ports(ssh_ports)
+                except RuntimeError as e:
+                    logger.error(f"Port verification failed: {e}")
+                    display.print_summary(
+                        PortVerificationResult(
+                            port=self.discovery_port,
+                            accessible=False,
+                            error=str(e)
+                        ),
+                        {}
+                    )
+                    return False
 
         # Add provider port as verified since we already checked it
         results[self.discovery_port] = PortVerificationResult(
@@ -237,7 +254,8 @@ class PortManager:
             used_ports = self._get_used_ports()
 
             # Find first available verified port
-            ports_to_check = sorted(self.verified_ports) if not self.skip_verification else range(self.start_port, self.end_port)
+            ports_to_check = sorted(list(self.verified_ports)) if not self.skip_verification else range(
+                self.start_port, self.end_port)
             for port in ports_to_check:
                 if port not in used_ports:
                     # Quick check if port is actually available
diff --git a/provider-server/provider/vm/proxy_manager.py b/provider-server/provider/vm/proxy_manager.py
index cec5f60..4fdf078 100644
--- a/provider-server/provider/vm/proxy_manager.py
+++ b/provider-server/provider/vm/proxy_manager.py
@@ -308,7 +308,7 @@ class PythonProxyManager:
         """
         try:
             # Use provided port or allocate one
-            if port is None or port == 0:
+            if port is None:
                 allocated_port = self.port_manager.allocate_port(vm_id)
                 if allocated_port is None:
                     logger.error(f"Failed to allocate port for VM {vm_id}")
diff --git a/provider-server/pyproject.toml b/provider-server/pyproject.toml
index 1ddcb1a..beeeb14 100644
--- a/provider-server/pyproject.toml
+++ b/provider-server/pyproject.toml
@@ -21,6 +21,7 @@ packages = [
 
 [tool.poetry.scripts]
 golem-provider = "provider.main:cli"
+dev = "provider.main:dev"
 
 [tool.poetry.dependencies]
 python = "^3.13"
@@ -44,10 +45,12 @@ httpx = "^0.23.0"
 typer = "^0.4.0"
 web3 = "==7.13.0"
 golem-base-sdk = "==0.1.0"
+dependency-injector = "^4.41.0"
 
 [tool.poetry.group.dev.dependencies]
 pytest = "^7.0.0"
 pytest-asyncio = "^0.18.0"
+pytest-mock = "^3.8.2"
 pytest-cov = "^3.0.0"
 black = "^22.3.0"
 isort = "^5.10.1"
diff --git a/provider-server/run.py b/provider-server/run.py
index 46bd849..88e5b81 100755
--- a/provider-server/run.py
+++ b/provider-server/run.py
@@ -6,96 +6,7 @@ import uvicorn
 from pathlib import Path
 from dotenv import load_dotenv
 
-from provider.utils.logging import setup_logger
-
-# Configure logging with debug mode
-logger = setup_logger(__name__, debug=True)
-
-async def verify_provider_port(port: int) -> bool:
-    """Verify that the provider port is available for binding.
-    
-    Args:
-        port: The port to verify
-        
-    Returns:
-        bool: True if the port is available, False otherwise
-    """
-    try:
-        # Try to create a temporary listener
-        server = await asyncio.start_server(
-            lambda r, w: None,  # Empty callback
-            '0.0.0.0',
-            port
-        )
-        server.close()
-        await server.wait_closed()
-        logger.info(f"✅ Provider port {port} is available")
-        return True
-    except Exception as e:
-        logger.error(f"❌ Provider port {port} is not available: {e}")
-        logger.error("Please ensure:")
-        logger.error(f"1. Port {port} is not in use by another application")
-        logger.error("2. You have permission to bind to this port")
-        logger.error("3. Your firewall allows binding to this port")
-        return False
-
-def check_requirements():
-    """Check if all requirements are met."""
-    try:
-        # Import settings to trigger validation
-        from provider.config import settings
-        return True
-    except Exception as e:
-        logger.error(f"Requirements check failed: {e}")
-        return False
-
-def main():
-    """Run the provider server."""
-    try:
-        # Load environment variables from .env.dev file if it exists, otherwise use .env
-        dev_env_path = Path(__file__).parent / '.env.dev'
-        env_path = dev_env_path if dev_env_path.exists() else Path(__file__).parent / '.env'
-        load_dotenv(dotenv_path=env_path)
-        logger.info(f"Loading environment variables from: {env_path}")
-
-        # Log environment variables
-        logger.info("Environment variables:")
-        for key, value in os.environ.items():
-            if key.startswith('GOLEM_PROVIDER_'):
-                logger.info(f"{key}={value}")
-
-        # Check requirements
-        if not check_requirements():
-            logger.error("Requirements check failed")
-            sys.exit(1)
-
-        # Import settings after loading environment variables
-        from provider.config import settings
-
-        # Verify provider port is available
-        if not asyncio.run(verify_provider_port(settings.PORT)):
-            logger.error(f"Provider port {settings.PORT} is not available")
-            sys.exit(1)
-
-        # Configure uvicorn logging
-        log_config = uvicorn.config.LOGGING_CONFIG
-        log_config["formatters"]["access"]["fmt"] = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
-        
-        # Run server
-        logger.process(f"🚀 Starting provider server on {settings.HOST}:{settings.PORT}")
-        uvicorn.run(
-            "provider:app",
-            host=settings.HOST,
-            port=settings.PORT,
-            reload=settings.DEBUG,
-            log_level="info" if not settings.DEBUG else "debug",
-            log_config=log_config,
-            timeout_keep_alive=60,  # Increase keep-alive timeout
-            limit_concurrency=100,  # Limit concurrent connections
-        )
-    except Exception as e:
-        logger.error(f"Failed to start provider server: {e}")
-        sys.exit(1)
+from provider.main import cli
 
 if __name__ == "__main__":
-    main()
+    cli()
diff --git a/requestor-server/requestor/services/ssh_service.py b/requestor-server/requestor/services/ssh_service.py
index 1b076f3..c05152f 100644
--- a/requestor-server/requestor/services/ssh_service.py
+++ b/requestor-server/requestor/services/ssh_service.py
@@ -36,7 +36,7 @@ class SSHService:
                 "-o", "UserKnownHostsFile=/dev/null",
                 f"{username}@{host}"
             ]
-            subprocess.run(cmd)
+            subprocess.run(cmd, check=True)
         except Exception as e:
             raise SSHError(f"Failed to establish SSH connection: {str(e)}")
 
